{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Getting started\n",
    "\n",
    "### CLEF 2025 - CheckThat! Lab  - Task 4 Scientific Web Discourse - Subtask 4b (Scientific Claim Source Retrieval)\n",
    "\n",
    "This notebook enables participants of subtask 4b to quickly get started. It includes the following:\n",
    "- Code to upload data, including:\n",
    "    - code to upload the collection set (CORD-19 academic papers' metadata)\n",
    "    - code to upload the query set (tweets with implicit references to CORD-19 papers)\n",
    "- Code to run a baseline retrieval model (BM25)\n",
    "- Code to evaluate the baseline model\n",
    "\n",
    "Participants are free to use this notebook and add their own models for the competition."
   ],
   "id": "20906a01f627b244"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1) Importing data",
   "id": "8f8916bc94c01274"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ],
   "id": "ece284233ed578e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "DATA_DIR = Path('../')",
   "id": "992851a73e28c5e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.a) Import the collection set\n",
    "The collection set contains metadata of CORD-19 academic papers.\n",
    "\n",
    "The preprocessed and filtered CORD-19 dataset is available on the Gitlab repository here: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "\n",
    "Participants should first download the file then upload it on the Google Colab session with the following steps.\n"
   ],
   "id": "43f5d2e77dfb5fb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) Download the collection set from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_COLLECTION_DATA = DATA_DIR /  'subtask4b_collection_data.pkl' #MODIFY PATH"
   ],
   "id": "a6ccbda67b1097e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)",
   "id": "961e67fe7d6baa00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_collection.info()",
   "id": "7947e33c41f977c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_collection.head()",
   "id": "fb2080a354bc94ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.b) Import the query set\n",
    "\n",
    "The query set contains tweets with implicit references to academic papers from the collection set.\n",
    "\n",
    "The preprocessed query set is available on the Gitlab repository here: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "\n",
    "Participants should first download the file then upload it on the Google Colab session with the following steps."
   ],
   "id": "f826da1061194396"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) Download the query tweets from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b?ref_type=heads\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_QUERY_TRAIN_DATA = DATA_DIR / 'subtask4b_query_tweets_train.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_DATA = DATA_DIR / 'subtask4b_query_tweets_dev.tsv' #MODIFY PATH"
   ],
   "id": "950034ac1bfc263a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')"
   ],
   "id": "ca68ade32035f3dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_train.head()",
   "id": "8f35f592ca48b1c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_train.info()",
   "id": "cc2abcc86a30641e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_dev.head()",
   "id": "1b3e0832c2010287"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_dev.info()",
   "id": "1d480fb34c2b094"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_train",
   "id": "f9490f1fb68741ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2) Trying BERT\n",
   "id": "b7bafd7a59017d67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader"
   ],
   "id": "1b44a16839645dd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)"
   ],
   "id": "a50ebea45368d7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_paper_text(df):\n",
    "    return (\n",
    "        df[\"title\"].fillna('') + \" \" +\n",
    "        df[\"abstract\"].fillna('')\n",
    "    )"
   ],
   "id": "6c37a089c7a57820"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_collection[\"paper_text\"] = build_paper_text(df_collection)",
   "id": "ae2f52336b4c80db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_train = df_query_train.merge(df_collection[[\"cord_uid\", \"paper_text\"]], on=\"cord_uid\")",
   "id": "cf79188354a6e84c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_examples = []\n",
    "for _, row in df_query_train.iterrows():\n",
    "    example = InputExample(texts=[row[\"tweet_text\"], row[\"paper_text\"]], label=1)\n",
    "    train_examples.append(example)"
   ],
   "id": "6bb093bf98acae18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)",
   "id": "a8785f56ce5beefc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_loss = losses.MultipleNegativesRankingLoss(model=model)",
   "id": "e697a465faf743f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_dev = df_query_train.merge(df_collection[[\"cord_uid\", \"paper_text\"]], on=\"cord_uid\")",
   "id": "955af509cb41c691"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dev_examples = []\n",
    "for _, row in df_query_dev.iterrows():\n",
    "    example = InputExample(texts=[row[\"tweet_text\"], row[\"paper_text\"]], label=1)\n",
    "    dev_examples.append(example)"
   ],
   "id": "b2ea5465aa4e8d7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dev_evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(dev_examples, name='dev-eval')",
   "id": "6894b0952e21900e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=1,\n",
    "    warmup_steps=100,\n",
    "    #steps_per_epoch=500,\n",
    "    output_path=f'fine-{model_name}',\n",
    "    save_best_model=True,\n",
    "    checkpoint_path=f'checkpoints/fine-tuned-{model_name}',\n",
    "    show_progress_bar=True,\n",
    "    optimizer_params={'lr': 2e-5}\n",
    ")"
   ],
   "id": "74d43b0ce00ff667"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "paper_texts = df_collection['title'] + ' ' + df_collection['abstract']\n",
    "paper_embeddings = model.encode(paper_texts.tolist(), convert_to_tensor=True)"
   ],
   "id": "a79e22f45204f207"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 1. Create the BERT corpus embeddings ---\n",
    "# Combine title + abstract like in BM25\n",
    "corpus = df_collection[['title', 'abstract']].apply(lambda x: f\"{x['title']} {x['abstract']}\", axis=1).tolist()\n",
    "cord_uids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "# Encode the corpus (can take time if large — do this once and cache)\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, progress_bar=True)\n",
    "\n",
    "# Optional: store cord_uid → index mapping\n",
    "uid_to_idx = {uid: i for i, uid in enumerate(cord_uids)}\n",
    "\n",
    "# --- 2. Encode a tweet and rank papers ---\n",
    "def retrieve_with_bert(tweet_text, top_k=10):\n",
    "    # Encode tweet\n",
    "    query_embedding = model.encode(tweet_text, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity with the corpus\n",
    "    scores = util.cos_sim(query_embedding, corpus_embeddings)[0]  # shape: (num_docs,)\n",
    "\n",
    "    # Get top-k highest scoring indices\n",
    "    top_results = torch.topk(scores, k=top_k)\n",
    "\n",
    "    # Retrieve corresponding paper IDs\n",
    "    top_indices = top_results.indices.cpu().tolist()\n",
    "    ranked_cord_uids = [cord_uids[i] for i in top_indices]\n",
    "\n",
    "    return ranked_cord_uids\n"
   ],
   "id": "4ad764b6d6b7d97a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text2berttop = {}\n",
    "\n",
    "def get_top_cord_uids_bert(query, top_k=10):\n",
    "    if query in text2berttop:\n",
    "        return text2berttop[query]\n",
    "    \n",
    "    # Encode the tweet query\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True, progress_bar=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "\n",
    "    # Get top-k indices\n",
    "    top_indices = torch.topk(scores, k=top_k).indices.cpu().tolist()\n",
    "\n",
    "    # Map indices to cord_uids\n",
    "    topk_uids = [cord_uids[i] for i in top_indices]\n",
    "\n",
    "    text2berttop[query] = topk_uids\n",
    "    return topk_uids"
   ],
   "id": "570929e1a4b25050"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 3. Apply to your datasets just like BM25 ---\n",
    "df_query_train['bert_topk'] = df_query_train['tweet_text'].apply(lambda x: get_top_cord_uids_bert(x))\n",
    "df_query_dev['bert_topk'] = df_query_dev['tweet_text'].apply(lambda x: get_top_cord_uids_bert(x))"
   ],
   "id": "e873b29d434de98e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3) Evaluating the baseline\n",
    "The following code evaluates the BM25 retrieval baseline on the query set using the Mean Reciprocal Rank score (MRR@5)."
   ],
   "id": "ed04079b5688eb27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        #performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance"
   ],
   "id": "ec41766e60c6fdd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'bert_topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'bert_topk')\n",
    "# Printed MRR@k results in the following format: {k: MRR@k}\n",
    "print(f\"Results on the train set: {results_train}\")\n",
    "print(f\"Results on the dev set: {results_dev}\")"
   ],
   "id": "fc2645d4655ed8ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4) Exporting results to prepare the submission on Codalab",
   "id": "49a8c08874d01844"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_dev['preds'] = df_query_dev['bm25_topk'].apply(lambda x: x[:5])",
   "id": "8ba11209a1653292"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_dev[['post_id', 'preds']].to_csv('predictions_bert.tsv', index=None, sep='\\t')",
   "id": "45dcd31edd11e236"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
