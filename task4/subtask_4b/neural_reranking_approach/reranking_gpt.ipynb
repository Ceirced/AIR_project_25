{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Getting started\n",
    "\n",
    "### CLEF 2025 - CheckThat! Lab  - Task 4 Scientific Web Discourse - Subtask 4b (Scientific Claim Source Retrieval)\n",
    "\n",
    "This notebook enables participants of subtask 4b to quickly get started. It includes the following:\n",
    "- Code to upload data, including:\n",
    "    - code to upload the collection set (CORD-19 academic papers' metadata)\n",
    "    - code to upload the query set (tweets with implicit references to CORD-19 papers)\n",
    "- Code to run a baseline retrieval model (BM25)\n",
    "- Code to evaluate the baseline model\n",
    "\n",
    "Participants are free to use this notebook and add their own models for the competition."
   ],
   "id": "d528d3629c14cfe5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1) Importing data",
   "id": "e720e90a98388596"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm.auto import tqdm"
   ],
   "id": "ded01119015bdd35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.a) Import the collection set\n",
    "The collection set contains metadata of CORD-19 academic papers.\n",
    "\n",
    "The preprocessed and filtered CORD-19 dataset is available on the Gitlab repository here: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "\n",
    "Participants should first download the file then upload it on the Google Colab session with the following steps.\n"
   ],
   "id": "18367de9760d8082"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) Download the collection set from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_COLLECTION_DATA = '../subtask4b_collection_data.pkl' #MODIFY PATH"
   ],
   "id": "be61cf090f8bd3dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)",
   "id": "6be2eb11b17af6e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_collection.info()",
   "id": "f6ef56271c96eeba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_collection.head()",
   "id": "8afa271c78ac7c5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.b) Import the query set\n",
    "\n",
    "The query set contains tweets with implicit references to academic papers from the collection set.\n",
    "\n",
    "The preprocessed query set is available on the Gitlab repository here: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "\n",
    "Participants should first download the file then upload it on the Google Colab session with the following steps."
   ],
   "id": "52e9b5570d09e278"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) Download the query tweets from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b?ref_type=heads\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_QUERY_TRAIN_DATA = '../subtask4b_query_tweets_train.tsv' #MODIFY PATH\n",
    "PATH_QUERY_DEV_DATA = '../subtask4b_query_tweets_dev.tsv' #MODIFY PATH"
   ],
   "id": "a89af15f8db1ba3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')"
   ],
   "id": "8022bc05ee60d45e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_train.head()",
   "id": "5dc47cb20c059123"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_train.info()",
   "id": "b10cd286a4094e12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_dev.head()",
   "id": "7a06498c1c4e6163"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_dev.info()",
   "id": "492954f7fe86284b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2) Running the baseline\n",
    "The following code runs a BM25 baseline.\n"
   ],
   "id": "e0ff000829be715e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from rank_bm25 import BM25Okapi",
   "id": "24d61ece5341f016"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create the BM25 corpus\n",
    "corpus = df_collection[:][['title', 'abstract']].apply(lambda x: f\"{x['title']} {x['abstract']}\", axis=1).tolist()\n",
    "cord_uids = df_collection[:]['cord_uid'].tolist()\n",
    "tokenized_corpus = [doc.split(' ') for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ],
   "id": "13fbbe20e79a09ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_top_cord_uids(query):\n",
    "  text2bm25top = {}\n",
    "  if query in text2bm25top.keys():\n",
    "      return text2bm25top[query]\n",
    "  else:\n",
    "      tokenized_query = query.split(' ')\n",
    "      doc_scores = bm25.get_scores(tokenized_query)\n",
    "      indices = np.argsort(-doc_scores)[:20]\n",
    "      bm25_topk = [cord_uids[x] for x in indices]\n",
    "\n",
    "      text2bm25top[query] = bm25_topk\n",
    "      return bm25_topk\n"
   ],
   "id": "ccbdaa3d0a638adf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Retrieve topk candidates using the BM25 model\n",
    "tqdm.pandas()\n",
    "#df_query_train['bm25_topk'] = df_query_train['tweet_text'].progress_apply(lambda x: get_top_cord_uids(x))\n",
    "df_query_dev['bm25_topk'] = df_query_dev['tweet_text'].progress_apply(lambda x: get_top_cord_uids(x))"
   ],
   "id": "5c399feed920fd87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_dev.head()",
   "id": "d971b3f81b5aac53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3) Evaluate retrieved candidates using MRR@k",
   "id": "ddefdc2679157558"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        #performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance"
   ],
   "id": "d1e5c6a01f757833"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4) Re-ranking approach",
   "id": "6cda3c03da88c13c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import openai\n",
    "import json"
   ],
   "id": "735443d62def4ca4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "openai.api_key = XXXXXXX\n",
    "GPT_MODEL    = \"o4-mini-2025-04-16\"\n",
    "TOP_K_FINAL       = 10\n",
    "BATCH_CLAIMS      = 50\n",
    "MAX_SNIPPET       = 200"
   ],
   "id": "88080f09ae4689c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def rerank_batch(batch_rows):\n",
    "    system = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a scientific IR specialist. \"\n",
    "            f\"For each claim, you will get a set of candidate paper excerpts with unique IDs. \"\n",
    "            f\"Your job is to select **exactly {TOP_K_FINAL}** excerpts per claim, ranked from most to least relevant. \"\n",
    "            \"Do NOT provide any explanations or extra text—only follow the user’s output format.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    parts = []\n",
    "    for _, (idx, claim, chunk_to_uid) in enumerate(batch_rows):\n",
    "        listing = \"\\n\".join(\n",
    "            f\"{j+1}. [{uid}] {chunk.replace(chr(10), ' ')[:MAX_SNIPPET]}…\"\n",
    "            for j, (chunk, uid) in enumerate(chunk_to_uid.items())\n",
    "        )\n",
    "        parts.append(\n",
    "            f\"---\\n\"\n",
    "            f\"ROW_INDEX: {idx}\\n\"\n",
    "            f\"Claim:\\n\\\"{claim}\\\"\\n\\n\"\n",
    "            \"Candidates:\\n\" + listing\n",
    "        )\n",
    "\n",
    "    user = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"\\n\\n\".join(parts)\n",
    "            + \"\\n\\n\"\n",
    "            \"Now output **one** JSON object (no commentary). \"\n",
    "            f\"Each key must be the ROW_INDEX as a string, and each value an **array of exactly {TOP_K_FINAL}** paper ID strings, \"\n",
    "            \"ordered from highest to lowest relevance. \"\n",
    "            \"If fewer than 10 are clearly relevant, still list 10 IDs by your best judgment. \"\n",
    "            \"Example:\\n\"\n",
    "            \"{\\n\"\n",
    "            \"  \\\"0\\\": [\\\"uid1\\\",\\\"uid2\\\", … ,\\\"uid10\\\"],\\n\"\n",
    "            \"  \\\"1\\\": [\\\"uidA\\\",\\\"uidB\\\", … ,\\\"uidJ\\\"]\\n\"\n",
    "            \"}\\n\"\n",
    "            \"Do NOT include any other keys, text, or formatting.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    resp = openai.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[system, user]\n",
    "    )\n",
    "    out = resp.choices[0].message.content.strip()\n",
    "\n",
    "    try:\n",
    "        raw = json.loads(out)\n",
    "\n",
    "        return {int(k): v for k, v in raw.items()}\n",
    "    except Exception as e:\n",
    "        print(\"JSON parse failed:\", e)\n",
    "        print(\"GPT output was:\", out)\n",
    "\n",
    "        return {\n",
    "            idx: list(chunk_to_uid.values())[:TOP_K_FINAL]\n",
    "            for idx, _, chunk_to_uid in batch_rows\n",
    "        }\n"
   ],
   "id": "e6f2fc3bdd8fd123"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_dev[\"gpt_topk\"] = None",
   "id": "377666139cbdee9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for batch_start in tqdm(range(0, len(df_query_dev), BATCH_CLAIMS)):\n",
    "    batch = []\n",
    "    for qi in range(batch_start, min(batch_start + BATCH_CLAIMS, len(df_query_dev))):\n",
    "        row    = df_query_dev.iloc[qi]\n",
    "        claim  = row[\"tweet_text\"]\n",
    "        uids   = row[\"bm25_topk\"][:20]\n",
    "\n",
    "        # build full-text->uid map\n",
    "        chunk_to_uid = {}\n",
    "        for uid in uids:\n",
    "            paper = df_collection.loc[df_collection[\"cord_uid\"] == uid]\n",
    "            if paper.empty:\n",
    "                continue\n",
    "            title    = paper[\"title\"].fillna(\"\").iloc[0].strip()\n",
    "            abstract = paper[\"abstract\"].fillna(\"\").iloc[0].strip()\n",
    "            full_txt = f\"{title}\\n\\n{abstract}\"\n",
    "            chunk_to_uid[full_txt] = uid\n",
    "\n",
    "        batch.append((qi, claim, chunk_to_uid))\n",
    "\n",
    "    results = rerank_batch(batch)\n",
    "\n",
    "    for idx, best_uids in results.items():\n",
    "        df_query_dev.at[idx, \"gpt_topk\"] = best_uids"
   ],
   "id": "fbc240606a3fecd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mask = df_query_dev['gpt_topk'].isnull()\n",
    "print(df_query_dev.loc[mask, ['cord_uid','gpt_topk']])"
   ],
   "id": "c05fc6c0f7178ba2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_query_dev.loc[mask, \"gpt_topk\"] = df_query_dev.loc[mask, \"bm25_topk\"]",
   "id": "80c546cbca0897cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_reranked = get_performance_mrr(df_query_dev, 'cord_uid', 'gpt_topk')\n",
    "print(f\"Results on the reranked set: {results_reranked}\")"
   ],
   "id": "27d484d04962b476"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_reranked = get_performance_mrr(df_query_dev, 'cord_uid', 'hybrid_topk')\n",
    "print(f\"Results on the reranked set: {results_reranked}\")"
   ],
   "id": "418f1356f40c7bab"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
