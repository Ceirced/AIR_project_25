{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee230c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rise/projects/clef2025-checkthat-lab/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, BertForSequenceClassification\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a76b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and prepare the data\n",
    "def load_data(tweets_file, papers_file):\n",
    "    \"\"\"Load tweet and paper data\"\"\"\n",
    "    tweets_df = pd.read_csv(tweets_file, sep='\\t')\n",
    "    papers_df = pd.read_pickle(papers_file)\n",
    "    \n",
    "    # Create a mapping from cord_uid to paper data\n",
    "    papers_dict = {}\n",
    "    for _, paper in papers_df.iterrows():\n",
    "        papers_dict[paper['cord_uid']] = {\n",
    "            'title': str(paper['title']),\n",
    "            'abstract': str(paper['abstract']) if pd.notna(paper['abstract']) else ''\n",
    "        }\n",
    "    \n",
    "    return tweets_df, papers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5ad1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetPaperMatchDataset(Dataset):\n",
    "    def __init__(self, tweets_df, papers_dict, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.tweet_texts = []\n",
    "        self.paper_texts = []\n",
    "        self.labels = []  # 1 for matching pairs, 0 for non-matching\n",
    "        \n",
    "        # Create positive samples (tweets with their matching papers)\n",
    "        for _, tweet in tweets_df.iterrows():\n",
    "            tweet_text = tweet['tweet_text']\n",
    "            cord_uid = tweet['cord_uid']\n",
    "            \n",
    "            if cord_uid in papers_dict:\n",
    "                paper = papers_dict[cord_uid]\n",
    "                paper_text = f\"{paper['title']} {paper['abstract']}\"\n",
    "                \n",
    "                self.tweet_texts.append(tweet_text)\n",
    "                self.paper_texts.append(paper_text)\n",
    "                self.labels.append(1)  # Positive sample\n",
    "        \n",
    "        # Create negative samples (tweets with random non-matching papers)\n",
    "        all_paper_ids = list(papers_dict.keys())\n",
    "        for _, tweet in tweets_df.iterrows():\n",
    "            tweet_text = tweet['tweet_text']\n",
    "            cord_uid = tweet['cord_uid']\n",
    "            \n",
    "            # Select a random non-matching paper\n",
    "            negative_candidates = [pid for pid in all_paper_ids if pid != cord_uid]\n",
    "            if negative_candidates:\n",
    "                random_paper_id = np.random.choice(negative_candidates)\n",
    "                paper = papers_dict[random_paper_id]\n",
    "                paper_text = f\"{paper['title']} {paper['abstract']}\"\n",
    "                \n",
    "                self.tweet_texts.append(tweet_text)\n",
    "                self.paper_texts.append(paper_text)\n",
    "                self.labels.append(0)  # Negative sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.tweet_texts[idx]\n",
    "        paper = self.paper_texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            tweet, \n",
    "            paper,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove the batch dimension\n",
    "        for key in encoding:\n",
    "            encoding[key] = encoding[key].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'],\n",
    "            'attention_mask': encoding['attention_mask'],\n",
    "            'token_type_ids': encoding.get('token_type_ids', torch.zeros_like(encoding['input_ids'])),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c19b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set up LoRA configuration\n",
    "def setup_lora_model(model_name=\"bert-base-uncased\", num_labels=2):\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    # Define LoRA Config\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8,  # Rank of the update matrices\n",
    "        lora_alpha=32,  # Alpha parameter for LoRA scaling\n",
    "        lora_dropout=0.1,  # Dropout probability for LoRA layers\n",
    "        # Target the attention modules in BERT\n",
    "        target_modules=[\"query\", \"key\", \"value\"],\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30595b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training function\n",
    "def train_model(model, train_dataloader, val_dataloader, device, epochs=3, lr=2e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.to(device)\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        train_accuracy = train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_accuracy = val_correct / val_total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_dataloader):.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss/len(val_dataloader):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            # Save only the LoRA weights - much more efficient\n",
    "            model.save_pretrained(\"./best_lora_tweet_paper_model\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8cfbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df, papers_dict = load_data(\"../subtask4b_query_tweets_dev.tsv\", \"../subtask4b_collection_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "852852cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "full_dataset = TweetPaperMatchDataset(tweets_df, papers_dict, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c72436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9808f0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Setup model with LoRA\n",
    "model = setup_lora_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = train_model(model, train_dataloader, val_dataloader, device, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
